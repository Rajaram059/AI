{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "6-3jpOooP_j6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hiL240cOuGP",
        "outputId": "f219021d-08dd-4dee-8759-38717e66eedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout\n",
        "from keras.optimizers import SGD\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "lem = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intents_file = open('intents.json').read()\n",
        "intents = json.loads(intents_file)"
      ],
      "metadata": {
        "id": "32yP4SBvfN6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the Data"
      ],
      "metadata": {
        "id": "4ZCU9BNt5HaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['!', '?', ',', '.']\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "       #tokenize each word\n",
        "        word = nltk.word_tokenize(pattern)\n",
        "        words.extend(word)\n",
        "        #add documents in the corpus\n",
        "        documents.append((word, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8SqpjEIfnEN",
        "outputId": "d388037c-0090-4d34-d1ac-36ed8c7529e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['Hi', 'there'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hey'], 'greeting'), (['Hola'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Nice', 'chatting', 'to', 'you', ',', 'bye'], 'goodbye'), (['Till', 'next', 'time'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['How', 'you', 'could', 'help', 'me', '?'], 'options'), (['What', 'you', 'can', 'do', '?'], 'options'), (['What', 'help', 'you', 'provide', '?'], 'options'), (['How', 'you', 'can', 'be', 'helpful', '?'], 'options'), (['What', 'support', 'is', 'offered'], 'options'), (['How', 'to', 'check', 'Adverse', 'drug', 'reaction', '?'], 'adverse_drug'), (['Open', 'adverse', 'drugs', 'module'], 'adverse_drug'), (['Give', 'me', 'a', 'list', 'of', 'drugs', 'causing', 'adverse', 'behavior'], 'adverse_drug'), (['List', 'all', 'drugs', 'suitable', 'for', 'patient', 'with', 'adverse', 'reaction'], 'adverse_drug'), (['Which', 'drugs', 'dont', 'have', 'adverse', 'reaction', '?'], 'adverse_drug'), (['Open', 'blood', 'pressure', 'module'], 'blood_pressure'), (['Task', 'related', 'to', 'blood', 'pressure'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'entry'], 'blood_pressure'), (['I', 'want', 'to', 'log', 'blood', 'pressure', 'results'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'management'], 'blood_pressure'), (['I', 'want', 'to', 'search', 'for', 'blood', 'pressure', 'result', 'history'], 'blood_pressure_search'), (['Blood', 'pressure', 'for', 'patient'], 'blood_pressure_search'), (['Load', 'patient', 'blood', 'pressure', 'result'], 'blood_pressure_search'), (['Show', 'blood', 'pressure', 'results', 'for', 'patient'], 'blood_pressure_search'), (['Find', 'blood', 'pressure', 'results', 'by', 'ID'], 'blood_pressure_search'), (['Find', 'me', 'a', 'pharmacy'], 'pharmacy_search'), (['Find', 'pharmacy'], 'pharmacy_search'), (['List', 'of', 'pharmacies', 'nearby'], 'pharmacy_search'), (['Locate', 'pharmacy'], 'pharmacy_search'), (['Search', 'pharmacy'], 'pharmacy_search'), (['Lookup', 'for', 'hospital'], 'hospital_search'), (['Searching', 'for', 'hospital', 'to', 'transfer', 'patient'], 'hospital_search'), (['I', 'want', 'to', 'search', 'hospital', 'data'], 'hospital_search'), (['Hospital', 'lookup', 'for', 'patient'], 'hospital_search'), (['Looking', 'up', 'hospital', 'details'], 'hospital_search')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmaztize and lower each word and remove duplicates\n",
        "words = [lem.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW2i0bYc80mV",
        "outputId": "ec164e1b-1580-46a3-9c11-951972920b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47 documents\n",
            "9 classes ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'options', 'pharmacy_search', 'thanks']\n",
            "87 unique lemmatized words [\"'s\", 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Training and Testing Data"
      ],
      "metadata": {
        "id": "4OqqHrdr-OJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the training data\n",
        "training = []\n",
        "# create empty array for the output\n",
        "output_empty = [0] * len(classes)\n",
        "# training set, bag of words for every sentence\n",
        "max_len = 0\n",
        "for doc in documents:\n",
        "    # initializing bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    word_patterns = doc[0]\n",
        "   # lemmatize each word - create base word, in attempt to represent related words\n",
        "    word_patterns = [lem.lemmatize(word.lower()) for word in word_patterns]\n",
        "    max_len = max(max_len, len(word_patterns))\n",
        "    for word in words:\n",
        "        bag.append(1) if word in word_patterns else bag.append(0)\n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "# shuffle the features and make numpy array\n",
        "random.shuffle(training)\n",
        "training = np.array(training,dtype = object)\n",
        "# create training and testing lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data is created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1qskK8Z9oE3",
        "outputId": "5068433a-b3cd-43ea-944b-4365874afbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data is created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model"
      ],
      "metadata": {
        "id": "PPUA8QvdA2mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deep neural networds model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "# Compiling model. SGD with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#Training and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "\n",
        "print(\"model is created\")"
      ],
      "metadata": {
        "id": "IDMIfriU-k0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interacting With the Chatbot"
      ],
      "metadata": {
        "id": "7ZToSHt1CYM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "model = load_model('chatbot_model.h5')\n",
        "import json\n",
        "import random\n",
        "intents = json.loads(open('intents.json').read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))\n",
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern - splitting words into array\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stemming every word - reducing to base form\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "# return bag of words array: 0 or 1 for words that exist in sentence\n",
        "def bag_of_words(sentence, words, show_details=True):\n",
        "    # tokenizing patterns\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words - vocabulary matrix\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,word in enumerate(words):\n",
        "            if word == s:\n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % word)\n",
        "    return(np.array(bag))\n",
        "def predict_class(sentence):\n",
        "    # filter below  threshold predictions\n",
        "\n",
        "    p = bag_of_words(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sorting strength probability\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list\n",
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Jie5NXBAYw",
        "outputId": "f622b51a-0283-47b1-fcf9-cd5f1f9425a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter\n",
        "from tkinter import *\n",
        "!pip install pyvirtualdisplay\n",
        "from pyvirtualdisplay import Display\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb\n",
        "\n",
        "# Start Xvfb\n",
        "display = Display(visible=0, size=(800, 600))\n",
        "display.start()\n",
        "\n",
        "# Set DISPLAY environment variable\n",
        "import os\n",
        "os.environ['DISPLAY'] = ':' + str(display.display) + '.0'\n",
        "def send():\n",
        "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
        "    EntryBox.delete(\"0.0\",END)\n",
        "    if msg != '':\n",
        "        ChatBox.config(state=NORMAL)\n",
        "        ChatBox.insert(END, \"You: \" + msg + '\\n\\n')\n",
        "        ChatBox.config(foreground=\"#446665\", font=(\"Verdana\", 12 ))\n",
        "        ints = predict_class(msg)\n",
        "        res = getResponse(ints, intents)\n",
        "        ChatBox.insert(END, \"Bot: \" + res + '\\n\\n')\n",
        "        ChatBox.config(state=DISABLED)\n",
        "        ChatBox.yview(END)\n",
        "root = Tk()\n",
        "root.title(\"Chatbot\")\n",
        "root.geometry(\"400x500\")\n",
        "root.resizable(width=FALSE, height=FALSE)\n",
        "#Create Chat window\n",
        "ChatBox = Text(root, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
        "ChatBox.config(state=DISABLED)\n",
        "#Bind scrollbar to Chat window\n",
        "scrollbar = Scrollbar(root, command=ChatBox.yview, cursor=\"heart\")\n",
        "ChatBox['yscrollcommand'] = scrollbar.set\n",
        "#Create Button to send message\n",
        "SendButton = Button(root, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
        "                    bd=0, bg=\"#f9a602\", activebackground=\"#3c9d9b\",fg='#000000',\n",
        "                    command= send )\n",
        "#Create the box to enter message\n",
        "EntryBox = Text(root, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
        "#EntryBox.bind(\"<Return>\", send)\n",
        "#Place all components on the screen\n",
        "scrollbar.place(x=376,y=6, height=386)\n",
        "ChatBox.place(x=6,y=6, height=386, width=370)\n",
        "EntryBox.place(x=128, y=401, height=90, width=265)\n",
        "SendButton.place(x=6, y=401, height=90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xEekxWRDDxf",
        "outputId": "34bd6f00-1cd4-42eb-e11f-40646389f999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,652 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,852 kB]\n",
            "Fetched 6,631 kB in 2s (2,798 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root.mainloop()"
      ],
      "metadata": {
        "id": "-FE63p8iTL0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4AMGgBC0TNRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AaKTrKOtNGxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}